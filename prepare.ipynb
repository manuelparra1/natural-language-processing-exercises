{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5bd42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2561b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = acquire.get_blog_articles(acquire.get_codeup_links())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599a9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = all_articles[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7754b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Black excellence in tech: Panelist Spotlight – Wilmarie De La Cruz Mejia\n",
      "\n",
      "Codeup is hosting a Black Excellence in Tech Panel in honor of Black History Month on February 22, 2023! To further celebrate, we’d like to spotlight each of our panelists leading up to the discussion to learn a bit about their respective experiences as black leaders in the tech industry!  \n",
      "Meet Wilmarie!\n",
      "Wilmarie De La Cruz Mejia is a current Codeup student on the path to becoming a Full-Stack Web Developer at our Dallas, TX campus. \n",
      "Wilmarie is a veteran expanding her knowledge of programming languages and technologies on her journey with Codeup. \n",
      "We asked Wilmarie to share more about her experience at Codeup. She shares, “I was able to meet other people who were passionate about coding and be in a positive learning environment.”\n",
      "We hope you can join us on February 22nd to sit in on an insightful conversation with Wilmarie and all of our panelists!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cca08",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86a16c",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "#### The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "#### In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3d1f3",
   "metadata": {},
   "source": [
    "### 1. Define a function named `basic_clean`. It should take in a string and apply some basic text cleaning to it:\n",
    "#### - Lowercase everything\n",
    "#### - Normalize unicode characters\n",
    "#### - Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    '''\n",
    "        This function accepts a string of text, cleans,\n",
    "        and then returns the cleaned text.\n",
    "    \n",
    "        params:\n",
    "        ------\n",
    "        string: Input text to clean\n",
    "\n",
    "        return:\n",
    "        ------\n",
    "        string: cleaned text\n",
    "    '''\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    \n",
    "    # only alphanumeric, apostrophe, & Spaces\n",
    "    text = re.sub(r\"[^a-z0-9'\\s]\", '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ece60",
   "metadata": {},
   "source": [
    "### 2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf42403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "        this function accepts a string of text, tokenizes,\n",
    "        and then returns the tokenized text.\n",
    "    \n",
    "        params:\n",
    "        ------\n",
    "        string: input text to tokenize\n",
    "\n",
    "        return:\n",
    "        ------\n",
    "        string: tokenized text\n",
    "    '''\n",
    "    \n",
    "    #create tokenizer object\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    #use the tokenizer\n",
    "    string = tokenizer.tokenize(text, return_str = True)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a040f",
   "metadata": {},
   "source": [
    "### 3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    '''\n",
    "        this function accepts a string of text, stems,\n",
    "        and then returns the stemmed string.\n",
    "    \n",
    "        params:\n",
    "        ------\n",
    "        string: input string to stem\n",
    "\n",
    "        return:\n",
    "        ------\n",
    "        string: stemmed string\n",
    "    '''\n",
    "    \n",
    "    #create stemmer object\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    #use the stem, split text using each word\n",
    "    stems = [ps.stem(word) for word in text.split()]\n",
    "    \n",
    "    #join stem word to text\n",
    "    text = ' '.join(stems)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feff2",
   "metadata": {},
   "source": [
    "### 4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb28d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    '''\n",
    "        this function accepts a string of text, lemmatizes,\n",
    "        and then returns the lemmatized string.\n",
    "    \n",
    "        params:\n",
    "        ------\n",
    "        text:\n",
    "            string: input string to lemmatize\n",
    "\n",
    "        return:\n",
    "        ------\n",
    "        text:\n",
    "            string: lemmatized string\n",
    "    '''\n",
    "    \n",
    "    #  create lemmatizer object\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # split text string into words\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    #join lemmatized words into article\n",
    "    text = ' '.join(lemmas)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475f159",
   "metadata": {},
   "source": [
    "### 5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.\n",
    "#### This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text,extra_words=None,exclude_words=None):\n",
    "    '''\n",
    "        this function accepts a string of text, removes stopwords,\n",
    "        and then returns the transformed string.\n",
    "    \n",
    "        params:\n",
    "        ------\n",
    "        text:\n",
    "            string: input string to transform\n",
    "\n",
    "        return:\n",
    "        ------\n",
    "        text:\n",
    "            string: transformed string\n",
    "    '''\n",
    "    #create stopword list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    #remove excluded words from list\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    \n",
    "    #add the extra words to the list\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    #split the string into different words\n",
    "    words = string.split()\n",
    "    \n",
    "    #create a list of words that are not in the list\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    #join the words that are not stopwords (filtered words) back into the string\n",
    "    text = ' '.join(filtered_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67446c",
   "metadata": {},
   "source": [
    "### 6. Use your data from the `acquire` to produce a dataframe of the news articles. Name the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({\"title\":titles,\"original\":originals,\"clean\":cleaned,\"stemmed\":stemmed,\"lemmatized\":lemmatized})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54eb53",
   "metadata": {},
   "source": [
    "### 7. Make another dataframe for the Codeup blog posts. Name the dataframe `codeup_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a900c0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'titles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m codeup_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mtitles\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m:originals,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m\"\u001b[39m:cleaned,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstemmed\u001b[39m\u001b[38;5;124m\"\u001b[39m:stemmed,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m\"\u001b[39m:lemmatized})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titles' is not defined"
     ]
    }
   ],
   "source": [
    "codeup_df = pd.DataFrame({\"title\":titles,\"original\":originals,\"clean\":cleaned,\"stemmed\":stemmed,\"lemmatized\":lemmatized})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30aa6bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codeup_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcodeup_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codeup_df' is not defined"
     ]
    }
   ],
   "source": [
    "codeup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580aad0d",
   "metadata": {},
   "source": [
    "### 8. For each dataframe, produce the following columns:\n",
    "#### - `title` to hold the title\n",
    "#### - `original` to hold the original article/post content\n",
    "#### - `clean` to hold the normalized and tokenized original with the stopwords removed.\n",
    "#### - `stemmed` to hold the stemmed version of the cleaned data.\n",
    "#### - `lemmatized` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    #original text from content column\n",
    "    df['original'] = df['content']\n",
    "    \n",
    "    #chain together clean, tokenize, remove stopwords\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    #chain clean, tokenize, stem, remove stopwords\n",
    "    df['stemmed'] = df['clean'].apply(stem)\n",
    "    \n",
    "    #clean clean, tokenize, lemmatize, remove stopwords\n",
    "    df['lemmatized'] = df['clean'].apply(lemmatize)\n",
    "    \n",
    "    return df[['title', 'original', 'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06593",
   "metadata": {},
   "source": [
    "### 9. Ask yourself:\n",
    "#### - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "#### - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "#### - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7a4b0",
   "metadata": {},
   "source": [
    "- lemmatized\n",
    "- stemmed\n",
    "- stemmed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
